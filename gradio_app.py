import gradio as gr
import os
import sys
import yaml
import librosa
import numpy as np
from datetime import datetime

# ============================================================================
# Ã‡ALIÅMA DÄ°ZÄ°NÄ° VE PYTHON PATH AYARLARI
# ============================================================================

# Ã‡alÄ±ÅŸma dizinini ayarla
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()
os.chdir(SCRIPT_DIR)

# Python path'e ekle - Bu Ã§ok Ã¶nemli!
if SCRIPT_DIR not in sys.path:
    sys.path.insert(0, SCRIPT_DIR)

print(f"ğŸ“ Ã‡alÄ±ÅŸma Dizini: {SCRIPT_DIR}")
print(f"ğŸ Python Path: {sys.path[:3]}")

OUTPUT_DIR = "gradio_outputs"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ============================================================================
# MODÃœLLERI Ä°Ã‡E AKTAR
# ============================================================================

try:
    from A2SB_lightning_module_api import TimePartitionedPretrainedSTFTBridgeModel
    from datasets.datamodule import STFTAudioDataModule
    print("âœ… ModÃ¼ller baÅŸarÄ±yla yÃ¼klendi")
except ImportError as e:
    print(f"âŒ ModÃ¼l yÃ¼kleme hatasÄ±: {e}")
    print("LÃ¼tfen doÄŸru dizinde olduÄŸunuzdan emin olun")

# ============================================================================
# YARDIMCI FONKSÄ°YONLAR
# ============================================================================

def compute_rolloff_freq(audio_file, roll_percent=0.99):
    """Otomatik cutoff frekansÄ± tespiti"""
    try:
        y, sr = librosa.load(audio_file, sr=None)
        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr, roll_percent=roll_percent)[0]
        return int(np.mean(rolloff))
    except Exception as e:
        print(f"Rolloff hesaplama hatasÄ±: {e}")
        return 2000

def kill_gpu_processes():
    """GPU'da Ã§alÄ±ÅŸan diÄŸer Python iÅŸlemlerini sonlandÄ±r"""
    try:
        import subprocess
        
        # nvidia-smi ile GPU kullanan iÅŸlemleri bul
        result = subprocess.run(
            ['nvidia-smi', '--query-compute-apps=pid', '--format=csv,noheader'],
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            pids = result.stdout.strip().split('\n')
            pids = [pid.strip() for pid in pids if pid.strip()]
            
            current_pid = os.getpid()
            for pid in pids:
                try:
                    pid_int = int(pid)
                    if pid_int != current_pid:
                        # Ä°ÅŸlemi sonlandÄ±r
                        if sys.platform == 'win32':
                            subprocess.run(['taskkill', '/F', '/PID', pid], 
                                         capture_output=True, check=False)
                        else:
                            subprocess.run(['kill', '-9', pid], 
                                         capture_output=True, check=False)
                        print(f"âŒ GPU iÅŸlemi sonlandÄ±rÄ±ldÄ± (PID: {pid})")
                except:
                    pass
    except:
        pass

def clear_gpu_memory():
    """GPU belleÄŸini temizle"""
    try:
        import gc
        import torch
        
        # Ã–nce diÄŸer iÅŸlemleri temizle
        kill_gpu_processes()
        
        # Python garbage collector
        gc.collect()
        
        if torch.cuda.is_available():
            # TÃ¼m GPU'larÄ± temizle
            for i in range(torch.cuda.device_count()):
                with torch.cuda.device(i):
                    torch.cuda.empty_cache()
                    torch.cuda.ipc_collect()
            
            torch.cuda.synchronize()
            print("ğŸ§¹ GPU belleÄŸi temizlendi")
            return True
        return False
    except Exception as e:
        print(f"âš ï¸ GPU bellek temizleme hatasÄ±: {e}")
        return False

def run_inference_direct(config_files, n_steps, output_path):
    """Inference'Ä± CLI ile Ã§alÄ±ÅŸtÄ±r - subprocess kullanarak"""
    try:
        import subprocess
        
        # GPU belleÄŸini temizle
        clear_gpu_memory()
        
        # PYTHONPATH'i environment variable olarak ayarla
        env = os.environ.copy()
        env['PYTHONPATH'] = SCRIPT_DIR
        
        # CUDA bellek optimizasyonlarÄ±
        env['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:128'
        env['CUDA_LAUNCH_BLOCKING'] = '0'
        
        # Colab iÃ§in Ã¶zel ayarlar
        try:
            import google.colab
            IN_COLAB = True
        except:
            IN_COLAB = False
        
        if IN_COLAB:
            env['COLAB_GPU'] = '1'

        # NCCL/IPC ayarlarÄ± (Colab gÃ¼venliÄŸi iÃ§in)
        env.setdefault('NCCL_P2P_DISABLE', '1')
        env.setdefault('NCCL_SHM_DISABLE', '1')
        env.setdefault('NCCL_IB_DISABLE', '1')

        # Config'teki predict_filelist uzunluÄŸunu oku (tek/multi dosya iÃ§in davranÄ±ÅŸÄ± belirle)
        num_predict_items = 1
        try:
            with open(config_files[1], 'r') as f:
                cfg_tmp = yaml.safe_load(f)
            num_predict_items = len(cfg_tmp.get('data', {}).get('predict_filelist', [])) or 1
        except Exception:
            pass

        # GPU sayÄ±sÄ±nÄ± tespit et ve uygun stratejiyi seÃ§
        try:
            import torch
            num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0
        except Exception:
            num_gpus = 0

        predict_bs = 8  # Bellek iÃ§in daha gÃ¼venli default

        if num_gpus >= 2:
            try:
                import torch
                free_per_gpu = []
                for idx in range(num_gpus):
                    try:
                        with torch.cuda.device(idx):
                            free, total = torch.cuda.mem_get_info()
                        free_per_gpu.append((idx, free))
                    except Exception:
                        free_per_gpu.append((idx, 0))
                # En Ã§ok boÅŸ belleÄŸe sahip GPU'lar
                free_per_gpu.sort(key=lambda x: x[1], reverse=True)

                if num_predict_items >= 2:
                    # Birden fazla dosya varsa Ã§oklu GPU (DDP) kullan
                    chosen = [str(free_per_gpu[i][0]) for i in range(min(2, len(free_per_gpu)))]
                    env['CUDA_VISIBLE_DEVICES'] = ','.join(chosen)
                    env.setdefault('MASTER_ADDR', '127.0.0.1')
                    env.setdefault('MASTER_PORT', '12975')
                    trainer_args = [
                        '--trainer.strategy=ddp',
                        f"--trainer.devices={len(chosen)}",
                        '--trainer.accelerator=gpu',
                        '--trainer.precision=16-mixed'
                    ]
                    print(f"ğŸš€ Ã‡oklu GPU DDP etkin: CUDA_VISIBLE_DEVICES={env['CUDA_VISIBLE_DEVICES']} | items={num_predict_items}")
                else:
                    # Tek dosyalÄ± tahminde en boÅŸ GPU'yu seÃ§ ve tek GPU kullan
                    best_gpu = str(free_per_gpu[0][0])
                    env['CUDA_VISIBLE_DEVICES'] = best_gpu
                    trainer_args = [
                        '--trainer.strategy=auto',
                        '--trainer.devices=1',
                        '--trainer.accelerator=gpu',
                        '--trainer.precision=16-mixed'
                    ]
                    print(f"ğŸ¯ Tek dosya: en boÅŸ GPU seÃ§ildi -> CUDA_VISIBLE_DEVICES={best_gpu}")
            except Exception:
                # Fallback: iki GPU da gÃ¶rÃ¼nÃ¼r, ancak tek GPU ile Ã§alÄ±ÅŸ
                env['CUDA_VISIBLE_DEVICES'] = '0,1'
                trainer_args = [
                    '--trainer.strategy=auto',
                    '--trainer.devices=1',
                    '--trainer.accelerator=gpu',
                    '--trainer.precision=16-mixed'
                ]
                print("âš ï¸ GPU seÃ§imi sÄ±rasÄ±nda hata: tek GPU fallback")
        elif num_gpus == 1:
            env['CUDA_VISIBLE_DEVICES'] = '0'
            trainer_args = [
                '--trainer.strategy=auto',
                '--trainer.devices=1',
                '--trainer.accelerator=gpu',
                '--trainer.precision=16-mixed'
            ]
            print("ğŸ¯ Tek GPU tespit edildi: CUDA_VISIBLE_DEVICES=0")
        else:
            trainer_args = [
                '--trainer.strategy=auto',
                '--trainer.devices=1',
                '--trainer.accelerator=cpu'
            ]
            print("ğŸ–¥ï¸ GPU tespit edilmedi: CPU modu")

        # Python'Ä± -u (unbuffered) flag'i ile Ã§alÄ±ÅŸtÄ±r
        cmd = [
            sys.executable,
            '-u',  # Unbuffered output
            os.path.join(SCRIPT_DIR, 'ensembled_inference_api.py'),
            'predict',
            '-c', config_files[0],
            '-c', config_files[1],
            f'--model.predict_n_steps={n_steps}',
            f'--model.output_audio_filename={output_path}',
            f'--model.predict_batch_size={predict_bs}',
        ] + trainer_args + [
            '--data.batch_size=1'  # Batch size'Ä± 1'e dÃ¼ÅŸÃ¼r
        ]
        
        print(f"ğŸ”§ Komut Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
        print(f"ğŸ“ PYTHONPATH: {env['PYTHONPATH']}")
        print(f"ğŸ“ Ã‡alÄ±ÅŸma dizini: {SCRIPT_DIR}")
        print(f"ğŸ“„ Config 1: {config_files[0]}")
        print(f"ğŸ“„ Config 2: {config_files[1]}")
        print(f"ğŸ“¤ Ã‡Ä±ktÄ±: {output_path}")
        print(f"ğŸ’» Komut: {' '.join(cmd)}")
        
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            cwd=SCRIPT_DIR,
            env=env
        )
        
        if result.returncode == 0:
            print(f"âœ… Inference tamamlandÄ±!")
            return True, None
        else:
            # Daha fazla hata detayÄ± gÃ¶ster
            error_msg = f"Hata kodu: {result.returncode}\n\n"
            error_msg += f"STDOUT (son 2000 karakter):\n{result.stdout[-2000:]}\n\n"
            error_msg += f"STDERR (son 2000 karakter):\n{result.stderr[-2000:]}"
            print(f"âŒ {error_msg}")
            return False, error_msg
        
    except Exception as e:
        import traceback
        error_msg = f"Hata: {str(e)}\n\n{traceback.format_exc()}"
        print(f"âŒ {error_msg}")
        return False, error_msg

def restore_audio(audio_file, mode, n_steps, cutoff_freq_auto, cutoff_freq_manual, 
                  inpaint_length, progress=gr.Progress()):
    """Ana ses restorasyon fonksiyonu"""
    try:
        progress(0, desc="ğŸš€ BaÅŸlatÄ±lÄ±yor...")
        
        if audio_file is None:
            return None, "âŒ LÃ¼tfen bir ses dosyasÄ± yÃ¼kleyin!"
        
        # Dosya adÄ± oluÅŸtur
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        input_filename = os.path.basename(audio_file)
        base_name = input_filename.rsplit('.', 1)[0] if '.' in input_filename else input_filename
        output_filename = f"{timestamp}_{mode}_{base_name}.wav"
        output_path = os.path.join(OUTPUT_DIR, output_filename)
        
        progress(0.1, desc="ğŸ“Š Ses dosyasÄ± analiz ediliyor...")
        
        # Ses bilgilerini al
        y, sr = librosa.load(audio_file, sr=None)
        duration = len(y) / sr
        
        info_text = f"## ğŸ“Š Ses Bilgileri\n\n"
        info_text += f"- **Sampling Rate:** {sr} Hz\n"
        info_text += f"- **Duration:** {duration:.2f} saniye\n"
        info_text += f"- **Samples:** {len(y):,}\n\n"
        
        # Config dosyalarÄ±nÄ± hazÄ±rla
        base_config = os.path.join(SCRIPT_DIR, 'configs', 'ensemble_2split_sampling.yaml')
        
        if mode == "bandwidth":
            progress(0.2, desc="ğŸ¯ Bandwidth extension hazÄ±rlanÄ±yor...")
            
            # Cutoff frekansÄ±nÄ± belirle
            if cutoff_freq_auto:
                cutoff_freq = compute_rolloff_freq(audio_file)
                info_text += f"ğŸ¯ **Otomatik Cutoff:** {cutoff_freq} Hz\n"
            else:
                cutoff_freq = int(cutoff_freq_manual)
                info_text += f"ğŸ¯ **Manuel Cutoff:** {cutoff_freq} Hz\n"
            
            info_text += f"âš™ï¸ **Sampling Steps:** {n_steps}\n\n"
            
            # Config hazÄ±rla
            config_path = os.path.join(SCRIPT_DIR, 'configs', 'inference_files_upsampling.yaml')
            if not os.path.exists(config_path):
                return None, f"âŒ Config dosyasÄ± bulunamadÄ±: {config_path}"
            
            with open(config_path, 'r') as f:
                config = yaml.safe_load(f)
            
            config['data']['predict_filelist'] = [{
                'filepath': audio_file,
                'output_subdir': '.'
            }]
            
            config['data']['transforms_aug'][0]['init_args']['upsample_mask_kwargs'] = {
                'min_cutoff_freq': cutoff_freq,
                'max_cutoff_freq': cutoff_freq
            }
            
            # Bellek optimizasyonu: segment_length'i azalt
            # Orijinal: 130560 samples (~3 saniye @ 44.1kHz)
            # Yeni: 65280 samples (~1.5 saniye @ 44.1kHz)
            if 'segment_length' in config['data']:
                config['data']['segment_length'] = 65280
                info_text += f"âš™ï¸ **Segment Length:** 65280 samples (~1.5s) - Bellek optimizasyonu\n"
            
            temp_config = os.path.join(SCRIPT_DIR, 'configs', f'temp_gradio_{timestamp}.yaml')
            with open(temp_config, 'w') as f:
                yaml.dump(config, f)
        
        elif mode == "inpainting":
            progress(0.2, desc="ğŸ¨ Audio inpainting hazÄ±rlanÄ±yor...")
            
            info_text += f"ğŸ¯ **Inpainting Length:** {inpaint_length}s\n"
            info_text += f"âš™ï¸ **Sampling Steps:** {n_steps}\n\n"
            
            # Config hazÄ±rla
            config_path = os.path.join(SCRIPT_DIR, 'configs', 'inference_files_inpainting.yaml')
            if not os.path.exists(config_path):
                return None, f"âŒ Config dosyasÄ± bulunamadÄ±: {config_path}"
            
            with open(config_path, 'r') as f:
                config = yaml.safe_load(f)
            
            config['data']['predict_filelist'] = [{
                'filepath': audio_file,
                'output_subdir': '.'
            }]
            
            config['data']['transforms_aug'][0]['init_args']['inpainting_mask_kwargs'] = {
                'min_inpainting_frac': inpaint_length,
                'max_inpainting_frac': inpaint_length,
                'is_random': False
            }
            
            temp_config = os.path.join(SCRIPT_DIR, 'configs', f'temp_gradio_{timestamp}.yaml')
            with open(temp_config, 'w') as f:
                yaml.dump(config, f)
        
        progress(0.3, desc="ğŸ”„ Model Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor... (Bu birkaÃ§ dakika sÃ¼rebilir)")
        
        # Inference Ã§alÄ±ÅŸtÄ±r
        success, error = run_inference_direct([base_config, temp_config], n_steps, output_path)
        
        # GeÃ§ici config'i sil
        if os.path.exists(temp_config):
            os.remove(temp_config)
        
        if not success:
            return None, f"## âŒ Hata\n\n```\n{error}\n```"
        
        progress(0.9, desc="âœ¨ TamamlanÄ±yor...")
        
        # Ã‡Ä±ktÄ± dosyasÄ±nÄ± kontrol et
        if os.path.exists(output_path):
            info_text += f"---\n\n## âœ… Ä°ÅŸlem TamamlandÄ±!\n\n"
            info_text += f"ğŸ“ **Ã‡Ä±ktÄ± DosyasÄ±:** `{output_filename}`\n\n"
            
            # Ã‡Ä±ktÄ± ses bilgileri
            y_out, sr_out = librosa.load(output_path, sr=None)
            info_text += f"## ğŸ“Š Restore EdilmiÅŸ Ses\n\n"
            info_text += f"- **Sampling Rate:** {sr_out} Hz\n"
            info_text += f"- **Duration:** {len(y_out)/sr_out:.2f} saniye\n"
            info_text += f"- **Samples:** {len(y_out):,}\n\n"
            
            # Spektral Ã¶zellikler
            cent_orig = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr)[0])
            cent_rest = np.mean(librosa.feature.spectral_centroid(y=y_out, sr=sr_out)[0])
            rolloff_orig = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr, roll_percent=0.99)[0])
            rolloff_rest = np.mean(librosa.feature.spectral_rolloff(y=y_out, sr=sr_out, roll_percent=0.99)[0])
            
            info_text += f"## ğŸ“ˆ Spektral Analiz\n\n"
            info_text += f"| Ã–zellik | Orijinal | Restored | DeÄŸiÅŸim |\n"
            info_text += f"|---------|----------|----------|---------|\\n"
            info_text += f"| Spectral Centroid | {cent_orig:.0f} Hz | {cent_rest:.0f} Hz | {((cent_rest-cent_orig)/cent_orig*100):+.1f}% |\n"
            info_text += f"| Spectral Rolloff | {rolloff_orig:.0f} Hz | {rolloff_rest:.0f} Hz | {((rolloff_rest-rolloff_orig)/rolloff_orig*100):+.1f}% |\n"
            
            progress(1.0, desc="âœ… TamamlandÄ±!")
            return output_path, info_text
        else:
            return None, info_text + "\n---\n\n## âŒ Hata\n\nÃ‡Ä±ktÄ± dosyasÄ± oluÅŸturulamadÄ±."
    
    except Exception as e:
        import traceback
        error_msg = f"## âŒ Hata OluÅŸtu\n\n```\n{str(e)}\n\n{traceback.format_exc()}\n```"
        return None, error_msg

# ============================================================================
# GRADIO ARAYÃœZÃœ
# ============================================================================

with gr.Blocks(title="A2SB Audio Restoration", theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # ğŸµ A2SB: Audio-to-Audio SchrÃ¶dinger Bridge
    ### YÃ¼ksek Kaliteli Ses Restorasyonu - NVIDIA
    
    Ses dosyalarÄ±nÄ±zÄ± AI ile restore edin!
    """)
    
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### ğŸ“¤ GiriÅŸ")
            
            audio_input = gr.Audio(
                label="Ses DosyasÄ± YÃ¼kle",
                type="filepath",
                sources=["upload", "microphone"]
            )
            
            mode = gr.Radio(
                choices=["bandwidth", "inpainting"],
                value="bandwidth",
                label="Restorasyon Modu"
            )
            
            with gr.Accordion("âš™ï¸ GeliÅŸmiÅŸ Ayarlar", open=False):
                n_steps = gr.Slider(25, 100, 50, step=5, label="Sampling Steps")
                cutoff_freq_auto = gr.Checkbox(True, label="Otomatik Cutoff")
                cutoff_freq_manual = gr.Slider(1000, 10000, 2000, step=100, label="Manuel Cutoff (Hz)", visible=False)
                inpaint_length = gr.Slider(0.1, 1.0, 0.3, step=0.1, label="Inpainting UzunluÄŸu (s)")
            
            cutoff_freq_auto.change(
                fn=lambda x: gr.update(visible=not x),
                inputs=[cutoff_freq_auto],
                outputs=[cutoff_freq_manual]
            )
            
            restore_btn = gr.Button("ğŸš€ Restore Et", variant="primary", size="lg")
        
        with gr.Column(scale=1):
            gr.Markdown("### ğŸ“¥ Ã‡Ä±ktÄ±")
            audio_output = gr.Audio(label="Restore EdilmiÅŸ Ses", type="filepath")
            info_output = gr.Markdown("Ses dosyasÄ± yÃ¼kleyin ve 'Restore Et' butonuna tÄ±klayÄ±n.")
    
    restore_btn.click(
        fn=restore_audio,
        inputs=[audio_input, mode, n_steps, cutoff_freq_auto, cutoff_freq_manual, inpaint_length],
        outputs=[audio_output, info_output]
    )

if __name__ == "__main__":
    print("\n" + "="*60)
    print("ğŸµ A2SB Audio Restoration")
    print("="*60)
    
    # Colab kontrolÃ¼
    try:
        import google.colab
        IN_COLAB = True
        print("ğŸŒ Google Colab ortamÄ±")
    except:
        IN_COLAB = False
        print("ğŸ’» Yerel ortam")
    
    print("\nğŸš€ Gradio baÅŸlatÄ±lÄ±yor...")
    print("="*60 + "\n")
    
    demo.launch(share=IN_COLAB, debug=True, show_error=True)
