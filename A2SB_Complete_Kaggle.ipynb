{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéµ A2SB: Audio-to-Audio Schr√∂dinger Bridge - Kaggle Edition\n",
    "\n",
    "**High-Quality Audio Restoration with NVIDIA A2SB**\n",
    "\n",
    "This notebook includes **everything** you need: model download, setup, and Gradio web interface!\n",
    "\n",
    "## ‚ö†Ô∏è IMPORTANT: GPU Required\n",
    "\n",
    "**This notebook requires Kaggle GPU acceleration!**\n",
    "\n",
    "### GPU Requirements:\n",
    "- üéØ **GPU Memory**: Requires 15GB+ GPU VRAM (P100 recommended)\n",
    "- ‚è±Ô∏è **Processing Time**: Long audio files need extended runtime\n",
    "- üíæ **RAM**: Minimum 25GB system RAM recommended\n",
    "- üöÄ **Performance**: P100 GPU recommended for best results\n",
    "\n",
    "### How to Enable GPU on Kaggle:\n",
    "1. Click **Settings** (right sidebar)\n",
    "2. Under **Accelerator**, select **GPU P100**\n",
    "3. Click **Save**\n",
    "\n",
    "### Kaggle Advantages:\n",
    "- ‚úÖ Free GPU access (30 hours/week)\n",
    "- ‚úÖ P100 GPU with 16GB VRAM\n",
    "- ‚úÖ 30GB RAM\n",
    "- ‚úÖ Persistent storage\n",
    "- ‚úÖ No subscription required\n",
    "\n",
    "---\n",
    "\n",
    "## üåü Features\n",
    "- ‚úÖ 44.1kHz high-resolution music restoration\n",
    "- ‚úÖ Bandwidth extension (high-frequency prediction)\n",
    "- ‚úÖ Audio inpainting (reconstruct missing segments)\n",
    "- ‚úÖ Support for long audio files (hours)\n",
    "- ‚úÖ End-to-end, no vocoder required\n",
    "- ‚úÖ **Gradio Web Interface** - User-friendly UI\n",
    "\n",
    "## üìö Resources\n",
    "- üìÑ [Paper](https://arxiv.org/abs/2501.11311)\n",
    "- üíª [GitHub Repository](https://github.com/test4373/diffusion-audio-restoration-colab-Kaggle-.git)\n",
    "- üé¨ [Original NVIDIA Demo](https://research.nvidia.com/labs/adlr/A2SB/)\n",
    "- ü§ó [Models](https://huggingface.co/nvidia/audio_to_audio_schrodinger_bridge)\n",
    "\n",
    "---\n",
    "\n",
    "**Usage:** Run cells in order. The last cell will launch the Gradio interface!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 1: Fix CUDA Compatibility (CRITICAL!)\n",
    "\n",
    "**‚ö†Ô∏è MUST RUN THIS FIRST!**\n",
    "\n",
    "Kaggle has PyTorch with CUDA 11.8, but torchvision with CUDA 12.4. This causes a version mismatch error.\n",
    "\n",
    "**This cell will:**\n",
    "1. Uninstall incompatible packages\n",
    "2. Install CUDA 11.8 compatible versions\n",
    "3. Fix the version mismatch\n",
    "\n",
    "**‚è±Ô∏è Takes 2-3 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üîß FIXING CUDA VERSION MISMATCH\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚ö†Ô∏è This is CRITICAL for Kaggle compatibility!\")\n",
    "print(\"‚è±Ô∏è  This will take 2-3 minutes...\\n\")\n",
    "\n",
    "# Step 1: Uninstall incompatible packages\n",
    "print(\"üì¶ Step 1/4: Removing incompatible packages...\")\n",
    "!pip uninstall -y torch torchvision torchaudio torchmetrics transformers torchao -q\n",
    "print(\"‚úì Old packages removed\\n\")\n",
    "\n",
    "# Step 2: Install PyTorch with CUDA 11.8 (compatible with Kaggle)\n",
    "print(\"üì¶ Step 2/4: Installing PyTorch with CUDA 11.8...\")\n",
    "!pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 \\\n",
    "    --index-url https://download.pytorch.org/whl/cu118 -q\n",
    "print(\"‚úì PyTorch packages installed\\n\")\n",
    "\n",
    "# Step 3: Install compatible torchmetrics (without transformers dependency)\n",
    "print(\"üì¶ Step 3/4: Installing torchmetrics...\")\n",
    "!pip install torchmetrics==1.2.0 --no-deps -q\n",
    "!pip install numpy packaging -q\n",
    "print(\"‚úì Torchmetrics installed\\n\")\n",
    "\n",
    "# Step 4: Install Lightning\n",
    "print(\"üì¶ Step 4/4: Installing Lightning framework...\")\n",
    "!pip install lightning==2.1.0 pytorch-lightning==2.1.0 -q\n",
    "print(\"‚úì Lightning installed\\n\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ CUDA FIX COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚ö†Ô∏è IMPORTANT: You MUST restart the kernel now!\")\n",
    "print(\"\\nüëâ Go to: Session > Restart Session\")\n",
    "print(\"\\nAfter restart, continue with Step 2 (skip this cell).\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è RESTART KERNEL NOW!\n",
    "\n",
    "**After running the cell above:**\n",
    "1. Go to **Session > Restart Session**\n",
    "2. Wait for kernel to restart\n",
    "3. Continue with Step 2 below\n",
    "4. **DO NOT** run Step 1 again\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Step 2: Verify Installation (After Kernel Restart)\n",
    "\n",
    "**Run this cell to verify the CUDA fix worked.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîç VERIFYING INSTALLATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check PyTorch versions\n",
    "print(f\"\\n‚úì PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úì PyTorch CUDA version: {torch.version.cuda}\")\n",
    "print(f\"‚úì Torchvision version: {torchvision.__version__}\")\n",
    "print(f\"‚úì CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\n‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úì GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Check if P100\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    if 'P100' in gpu_name:\n",
    "        print(\"\\nüéâ Perfect! You have a P100 GPU!\")\n",
    "    elif 'T4' in gpu_name:\n",
    "        print(\"\\n‚úì T4 GPU detected - Good for this notebook\")\n",
    "    elif 'V100' in gpu_name:\n",
    "        print(\"\\nüéâ Excellent! You have a V100 GPU!\")\n",
    "    \n",
    "    # Check memory\n",
    "    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    if gpu_memory_gb < 14:\n",
    "        print(f\"\\n‚ö†Ô∏è Warning: GPU has only {gpu_memory_gb:.1f} GB memory\")\n",
    "        print(\"Recommended: Enable P100 GPU (16GB) in Settings\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ GPU memory is sufficient ({gpu_memory_gb:.1f} GB)\")\n",
    "else:\n",
    "    print(\"\\n‚ùå ERROR: CUDA not available!\")\n",
    "    print(\"Please enable GPU in Settings > Accelerator > GPU P100\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Check CUDA compatibility\n",
    "pytorch_cuda = torch.version.cuda\n",
    "if pytorch_cuda == \"11.8\":\n",
    "    print(f\"\\n‚úÖ CUDA version is correct: {pytorch_cuda}\")\n",
    "    print(\"‚úÖ No version mismatch - Ready to proceed!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Warning: PyTorch CUDA version is {pytorch_cuda}\")\n",
    "    print(\"Expected: 11.8\")\n",
    "\n",
    "# Check RAM\n",
    "try:\n",
    "    with open('/proc/meminfo', 'r') as f:\n",
    "        meminfo = f.read()\n",
    "    mem_total = int([line for line in meminfo.split('\\n') if 'MemTotal' in line][0].split()[1]) / 1024 / 1024\n",
    "    print(f\"\\n‚úì System RAM: {mem_total:.1f} GB\")\n",
    "    if mem_total < 20:\n",
    "        print(\"‚ö†Ô∏è Low RAM - May have issues with large audio files\")\n",
    "    else:\n",
    "        print(\"‚úÖ RAM is sufficient\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ VERIFICATION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nYou can now proceed with the rest of the notebook.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 3: Install Dependencies\n",
    "\n",
    "**Install remaining packages needed for audio restoration.**\n",
    "\n",
    "**‚è±Ô∏è Takes 3-5 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"üì¶ Installing dependencies...\\n\")\n",
    "\n",
    "# Audio processing libraries\n",
    "print(\"üì• Installing audio processing libraries...\")\n",
    "!pip install librosa soundfile scipy einops -q\n",
    "print(\"‚úì Audio libraries installed\\n\")\n",
    "\n",
    "# Configuration tools\n",
    "print(\"üì• Installing configuration tools...\")\n",
    "!pip install jsonargparse[signatures] pyyaml -q\n",
    "print(\"‚úì Configuration tools installed\\n\")\n",
    "\n",
    "# Fix huggingface_hub compatibility for Gradio\n",
    "print(\"üì• Installing Gradio and utilities...\")\n",
    "!pip install 'huggingface_hub>=0.19.0,<1.0' -q\n",
    "!pip install gradio==4.44.0 -q\n",
    "!pip install nest-asyncio tqdm -q\n",
    "!pip install rotary-embedding-torch -q\n",
    "!pip install pyngrok -q\n",
    "print(\"‚úì Gradio and utilities installed\\n\")\n",
    "\n",
    "# Optional: SSR Eval\n",
    "print(\"üì• Installing optional packages...\")\n",
    "!pip install ssr-eval -q 2>/dev/null || echo \"‚ö†Ô∏è SSR Eval skipped (optional)\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ ALL DEPENDENCIES INSTALLED!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verify key packages\n",
    "import torch\n",
    "import lightning\n",
    "import gradio as gr\n",
    "import librosa\n",
    "import nest_asyncio\n",
    "\n",
    "# Fix event loop issue\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(f\"\\n‚úì PyTorch: {torch.__version__}\")\n",
    "print(f\"‚úì Lightning: {lightning.__version__}\")\n",
    "print(f\"‚úì Gradio: {gr.__version__}\")\n",
    "print(f\"‚úì Librosa: {librosa.__version__}\")\n",
    "print(f\"‚úì CUDA: {torch.version.cuda}\")\n",
    "\n",
    "# Check huggingface_hub\n",
    "try:\n",
    "    import huggingface_hub\n",
    "    print(f\"‚úì HuggingFace Hub: {huggingface_hub.__version__}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"\\nüéâ Ready to proceed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Step 4: Clone Repository\n",
    "\n",
    "**Download the audio restoration code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"üì• Cloning repository...\\n\")\n",
    "\n",
    "# Change to /kaggle/working directory (writable)\n",
    "os.chdir('/kaggle/working')\n",
    "\n",
    "# Remove if exists\n",
    "!rm -rf diffusion-audio-restoration-colab-Kaggle-\n",
    "\n",
    "# Clone repository\n",
    "!git clone https://github.com/test4373/diffusion-audio-restoration-colab-Kaggle-.git\n",
    "os.chdir('diffusion-audio-restoration-colab-Kaggle-')\n",
    "\n",
    "print(f\"\\n‚úÖ Repository cloned successfully!\")\n",
    "print(f\"‚úì Current directory: {os.getcwd()}\")\n",
    "\n",
    "# List key files\n",
    "print(\"\\nüìÇ Key files:\")\n",
    "!ls -la | grep -E \"(gradio|config|py$)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ó Step 5: Download Pretrained Models\n",
    "\n",
    "**Download two model checkpoints from Hugging Face:**\n",
    "- One-split (0.0-1.0): ~1.5GB\n",
    "- Two-split (0.5-1.0): ~1.5GB\n",
    "\n",
    "**Total: ~3GB. Takes 5-10 minutes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "print(\"ü§ó Downloading pretrained models from Hugging Face...\")\n",
    "print(\"‚è±Ô∏è  This will take 5-10 minutes...\\n\")\n",
    "\n",
    "model_dir = './pretrained_models'\n",
    "\n",
    "try:\n",
    "    snapshot_download(\n",
    "        repo_id='nvidia/audio_to_audio_schrodinger_bridge',\n",
    "        local_dir=model_dir,\n",
    "        local_dir_use_symlinks=False\n",
    "    )\n",
    "    print(f\"\\n‚úÖ Models downloaded to: {model_dir}\")\n",
    "    \n",
    "    # Verify download\n",
    "    if os.path.exists(model_dir):\n",
    "        print(f\"\\nüìÇ Model files:\")\n",
    "        !ls -lh {model_dir}\n",
    "    \n",
    "    print(\"\\n‚úÖ Model download complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error downloading models: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Check your internet connection\")\n",
    "    print(\"2. Verify Hugging Face is accessible\")\n",
    "    print(\"3. Try running this cell again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë Step 6a: Setup Ngrok Token (Optional but Recommended)\n",
    "\n",
    "**Ngrok provides a stable public URL for your Gradio interface.**\n",
    "\n",
    "### How to get Ngrok token:\n",
    "1. Go to [ngrok.com](https://ngrok.com/)\n",
    "2. Sign up for free account\n",
    "3. Go to [Dashboard](https://dashboard.ngrok.com/get-started/your-authtoken)\n",
    "4. Copy your authtoken\n",
    "5. Paste it below\n",
    "\n",
    "**Note:** Without ngrok token, Gradio will use its default sharing (also works fine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Launch Gradio interface\n",
    "print(\"üöÄ Launching Gradio interface...\\n\")\n",
    "print(\"‚è±Ô∏è  Please wait for the link to appear below.\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use Kaggle-optimized Gradio app with ngrok\n",
    "import os\n",
    "if os.path.exists('gradio_app_kaggle.py'):\n",
    "    print(\"‚úì Using Kaggle-optimized Gradio app\")\n",
    "    !python gradio_app_kaggle.py --ngrok\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Using default Gradio app\")\n",
    "    !python gradio_app.py --share\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Gradio interface launched!\")\n",
    "print(\"Click the public link above to access the web interface.\")\n",
    "print(\"The link will be valid for 72 hours.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Troubleshooting\n",
    "\n",
    "### ‚ùå CUDA Out of Memory\n",
    "\n",
    "**Run this cell to clear GPU memory:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "print(\"üßπ Clearing GPU memory...\")\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Show memory status\n",
    "    total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    cached = torch.cuda.memory_reserved(0) / 1e9\n",
    "    \n",
    "    print(f\"\\n‚úÖ GPU memory cleared!\")\n",
    "    print(f\"\\nMemory Status:\")\n",
    "    print(f\"  Total: {total:.2f} GB\")\n",
    "    print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"  Cached: {cached:.2f} GB\")\n",
    "    print(f\"  Free: {total - allocated:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç System Diagnostics\n",
    "\n",
    "**Run this if you encounter issues:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"üîç System Diagnostics\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Python info\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "\n",
    "# PyTorch info\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"PyTorch CUDA version: {torch.version.cuda}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Memory info\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    cached = torch.cuda.memory_reserved(0) / 1e9\n",
    "    \n",
    "    print(f\"\\nGPU Memory:\")\n",
    "    print(f\"  Total: {total_memory:.2f} GB\")\n",
    "    print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"  Cached: {cached:.2f} GB\")\n",
    "    print(f\"  Free: {total_memory - allocated:.2f} GB\")\n",
    "\n",
    "# Check installed packages\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Key Package Versions:\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "packages = [\n",
    "    'torch', 'torchvision', 'torchaudio', 'torchmetrics',\n",
    "    'lightning', 'pytorch-lightning', 'gradio', 'librosa'\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        mod = __import__(package.replace('-', '_'))\n",
    "        version = getattr(mod, '__version__', 'unknown')\n",
    "        print(f\"‚úì {package}: {version}\")\n",
    "    except ImportError:\n",
    "        print(f\"‚úó {package}: NOT INSTALLED\")\n",
    "\n",
    "# Check working directory\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Directory exists: {os.path.exists(os.getcwd())}\")\n",
    "\n",
    "# Check for key files\n",
    "key_files = [\n",
    "    'gradio_app_kaggle.py',\n",
    "    'A2SB_lightning_module_api.py',\n",
    "    'ensembled_inference_api.py',\n",
    "    'configs/ensemble_2split_sampling.yaml'\n",
    "]\n",
    "\n",
    "print(f\"\\nKey files:\")\n",
    "for file in key_files:\n",
    "    exists = \"‚úì\" if os.path.exists(file) else \"‚úó\"\n",
    "    print(f\"{exists} {file}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Tips and Best Practices\n",
    "\n",
    "### ‚ö° Performance Tips\n",
    "\n",
    "**Processing Times (P100 GPU):**\n",
    "- 10s audio, 50 steps: ~2-3 minutes\n",
    "- 30s audio, 50 steps: ~5-7 minutes\n",
    "- 60s audio, 50 steps: ~10-15 minutes\n",
    "\n",
    "**Quality Settings:**\n",
    "- **25-30 steps**: Fast (good quality)\n",
    "- **50-75 steps**: Balanced (excellent) ‚≠ê Recommended\n",
    "- **75-100 steps**: Best (outstanding)\n",
    "\n",
    "### üí° Best Practices\n",
    "\n",
    "1. **Start small**: Test with 10-20 second clips\n",
    "2. **Use defaults**: 50 steps, auto cutoff\n",
    "3. **Monitor GPU**: Check memory if issues occur\n",
    "4. **Save outputs**: Download immediately\n",
    "5. **Batch processing**: One file at a time\n",
    "\n",
    "### üÜò Common Issues\n",
    "\n",
    "**CUDA Out of Memory:**\n",
    "- Reduce sampling steps to 25-30\n",
    "- Use shorter audio clips\n",
    "- Run the GPU memory clear cell above\n",
    "- Restart kernel\n",
    "\n",
    "**Import Errors:**\n",
    "- Make sure you ran Step 1 and restarted kernel\n",
    "- Re-run Step 3 (dependencies)\n",
    "\n",
    "**Model Not Found:**\n",
    "- Re-run Step 5 (model download)\n",
    "- Check internet connection\n",
    "\n",
    "### üìÅ File Locations\n",
    "\n",
    "- **Input**: Upload via Gradio or `/kaggle/input/`\n",
    "- **Output**: `/kaggle/working/gradio_outputs/`\n",
    "- **Models**: `/kaggle/working/diffusion-audio-restoration-colab-Kaggle-/pretrained_models/`\n",
    "\n",
    "### üìñ Resources\n",
    "\n",
    "- **Paper**: [arXiv:2501.11311](https://arxiv.org/abs/2501.11311)\n",
    "- **GitHub**: [Repository](https://github.com/test4373/diffusion-audio-restoration-colab-Kaggle-.git)\n",
    "- **Models**: [HuggingFace](https://huggingface.co/nvidia/audio_to_audio_schrodinger_bridge)\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Thank You!\n",
    "\n",
    "**Citation:**\n",
    "```bibtex\n",
    "@article{kong2025a2sb,\n",
    "  title={A2SB: Audio-to-Audio Schrodinger Bridges},\n",
    "  author={Kong, Zhifeng and Shih, Kevin J and Nie, Weili and Vahdat, Arash and Lee, Sang-gil and Santos, Joao Felipe and Jukic, Ante and Valle, Rafael and Catanzaro, Bryan},\n",
    "  journal={arXiv preprint arXiv:2501.11311},\n",
    "  year={2025}\n",
    "}\n",
    "```\n",
    "\n",
    "### ‚≠ê Support This Project\n",
    "\n",
    "- ‚≠ê Star the [GitHub repository](https://github.com/test4373/diffusion-audio-restoration-colab-Kaggle-.git)\n",
    "- üêõ Report bugs or suggest features\n",
    "- üì¢ Share with others\n",
    "- üëç Upvote this notebook\n",
    "\n",
    "---\n",
    "\n",
    "**Made with ‚ù§Ô∏è for the audio restoration community**\n",
    "\n",
    "**Optimized for Kaggle with CUDA compatibility fix and GPU memory management**"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
