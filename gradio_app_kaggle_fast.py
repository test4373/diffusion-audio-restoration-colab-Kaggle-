import gradio as gr
import os
import sys
import yaml
import librosa
import numpy as np
from datetime import datetime
import argparse
import torch

# ============================================================================
# KAGGLE ORTAM AYARLARI
# ============================================================================

# Kaggle ortamÄ±nÄ± tespit et
IN_KAGGLE = os.path.exists('/kaggle/working')

if IN_KAGGLE:
    # Kaggle'da Ã§alÄ±ÅŸma dizini
    SCRIPT_DIR = '/kaggle/working/diffusion-audio-restoration-colab-Kaggle-'
    OUTPUT_DIR = '/kaggle/working/gradio_outputs'
    print("ğŸŒ Kaggle ortamÄ± tespit edildi")
else:
    # Yerel ortam
    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()
    OUTPUT_DIR = "gradio_outputs"
    print("ğŸ’» Yerel ortam")

# Ã‡alÄ±ÅŸma dizinini ayarla
os.chdir(SCRIPT_DIR)

# Python path'e ekle
if SCRIPT_DIR not in sys.path:
    sys.path.insert(0, SCRIPT_DIR)

print(f"ğŸ“ Ã‡alÄ±ÅŸma Dizini: {SCRIPT_DIR}")
print(f"ğŸ“ Ã‡Ä±ktÄ± Dizini: {OUTPUT_DIR}")
print(f"ğŸ Python Path: {sys.path[:3]}")

os.makedirs(OUTPUT_DIR, exist_ok=True)

# ============================================================================
# GRADIO API PATCH
# ============================================================================

def patch_gradio_client():
    """Gradio 4.44.0'daki JSON schema hatalarÄ±nÄ± dÃ¼zelt"""
    try:
        import gradio_client.utils as client_utils
        
        original_json_schema_to_python_type = client_utils._json_schema_to_python_type
        
        def patched_json_schema_to_python_type(schema, defs=None):
            try:
                if isinstance(schema, bool):
                    return "Any" if schema else "None"
                return original_json_schema_to_python_type(schema, defs)
            except (TypeError, AttributeError, KeyError) as e:
                print(f"âš ï¸ JSON schema hatasÄ± dÃ¼zeltildi: {type(e).__name__}")
                return "Any"
        
        client_utils._json_schema_to_python_type = patched_json_schema_to_python_type
        print("âœ… Gradio client API patch baÅŸarÄ±yla uygulandÄ±")
        return True
        
    except Exception as e:
        print(f"âš ï¸ Gradio patch uygulanamadÄ± (gÃ¶z ardÄ± edildi): {e}")
        return False

patch_gradio_client()

# ============================================================================
# MODÃœLLERI Ä°Ã‡E AKTAR
# ============================================================================

try:
    from A2SB_lightning_module_fast import FastTimePartitionedPretrainedSTFTBridgeModel
    from datasets.datamodule import STFTAudioDataModule
    from fast_inference_optimizer import FastInferenceOptimizer
    print("âœ… Fast inference modÃ¼lleri baÅŸarÄ±yla yÃ¼klendi")
except ImportError as e:
    print(f"âŒ ModÃ¼l yÃ¼kleme hatasÄ±: {e}")
    print("LÃ¼tfen doÄŸru dizinde olduÄŸunuzdan emin olun")

# ============================================================================
# YARDIMCI FONKSÄ°YONLAR
# ============================================================================

def compute_rolloff_freq(audio_file, roll_percent=0.99):
    """Otomatik cutoff frekansÄ± tespiti"""
    try:
        y, sr = librosa.load(audio_file, sr=None)
        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr, roll_percent=roll_percent)[0]
        return int(np.mean(rolloff))
    except Exception as e:
        print(f"Rolloff hesaplama hatasÄ±: {e}")
        return 2000

def kill_gpu_processes():
    """GPU'da Ã§alÄ±ÅŸan diÄŸer Python iÅŸlemlerini sonlandÄ±r"""
    try:
        import subprocess
        result = subprocess.run(
            ['nvidia-smi', '--query-compute-apps=pid', '--format=csv,noheader'],
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            pids = result.stdout.strip().split('\n')
            pids = [pid.strip() for pid in pids if pid.strip()]
            
            current_pid = os.getpid()
            for pid in pids:
                try:
                    pid_int = int(pid)
                    if pid_int != current_pid:
                        subprocess.run(['kill', '-9', pid], 
                                     capture_output=True, check=False)
                        print(f"âŒ GPU iÅŸlemi sonlandÄ±rÄ±ldÄ± (PID: {pid})")
                except:
                    pass
    except:
        pass

def clear_gpu_memory():
    """GPU belleÄŸini temizle"""
    try:
        import gc
        kill_gpu_processes()
        gc.collect()
        
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                with torch.cuda.device(i):
                    torch.cuda.empty_cache()
                    torch.cuda.ipc_collect()
            torch.cuda.synchronize()
            print("ğŸ§¹ GPU belleÄŸi temizlendi")
            return True
        return False
    except Exception as e:
        print(f"âš ï¸ GPU bellek temizleme hatasÄ±: {e}")
        return False

def run_inference_fast(config_files, n_steps, output_path, use_compile=True, 
                       precision='fp16', compile_mode='reduce-overhead'):
    """HÄ±zlÄ± inference Ã§alÄ±ÅŸtÄ±r"""
    try:
        import subprocess
        import time
        
        clear_gpu_memory()
        
        env = os.environ.copy()
        env['PYTHONPATH'] = SCRIPT_DIR
        env['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:128'
        env['CUDA_LAUNCH_BLOCKING'] = '0'
        
        if IN_KAGGLE:
            env['KAGGLE_KERNEL_RUN_TYPE'] = 'Interactive'

        env.setdefault('NCCL_P2P_DISABLE', '1')
        env.setdefault('NCCL_SHM_DISABLE', '1')
        env.setdefault('NCCL_IB_DISABLE', '1')

        num_predict_items = 1
        try:
            with open(config_files[1], 'r') as f:
                cfg_tmp = yaml.safe_load(f)
            num_predict_items = len(cfg_tmp.get('data', {}).get('predict_filelist', [])) or 1
        except Exception:
            pass

        try:
            num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0
        except Exception:
            num_gpus = 0

        predict_bs = 16  # HÄ±zlÄ± inference iÃ§in artÄ±rÄ±ldÄ±

        if num_gpus >= 1:
            env['CUDA_VISIBLE_DEVICES'] = '0'
            trainer_args = [
                '--trainer.strategy=auto',
                '--trainer.devices=1',
                '--trainer.accelerator=gpu',
                '--trainer.precision=16-mixed'
            ]
            print("ğŸ¯ GPU modu: CUDA_VISIBLE_DEVICES=0")
        else:
            trainer_args = [
                '--trainer.strategy=auto',
                '--trainer.devices=1',
                '--trainer.accelerator=cpu'
            ]
            print("ğŸ–¥ï¸ CPU modu")

        # HÄ±zlÄ± inference parametreleri
        fast_params = [
            '--model.use_fast_inference=True',
            f'--model.use_compile={use_compile}',
            f'--model.precision={precision}',
            f'--model.compile_mode={compile_mode}',
            '--model.use_cudnn_benchmark=True',
            '--model.use_tf32=True'
        ]

        cmd = [
            sys.executable,
            '-u',
            os.path.join(SCRIPT_DIR, 'ensembled_inference_fast_api.py'),
            'predict',
            '-c', config_files[0],
            '-c', config_files[1],
            f'--model.predict_n_steps={n_steps}',
            f'--model.output_audio_filename={output_path}',
            f'--model.predict_batch_size={predict_bs}',
        ] + trainer_args + fast_params + [
            '--data.batch_size=1'
        ]
        
        print(f"\n{'='*60}")
        print("ğŸš€ FAST INFERENCE MODE (KAGGLE)")
        print(f"{'='*60}")
        print(f"âœ“ Compile: {use_compile}")
        print(f"âœ“ Precision: {precision}")
        print(f"âœ“ Compile mode: {compile_mode}")
        print(f"âœ“ Steps: {n_steps}")
        print(f"âœ“ Batch size: {predict_bs}")
        print(f"{'='*60}\n")
        
        start_time = time.time()
        
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            cwd=SCRIPT_DIR,
            env=env,
            timeout=1800  # 30 dakika timeout
        )
        
        end_time = time.time()
        elapsed_time = end_time - start_time
        
        if result.returncode == 0:
            print(f"\n{'='*60}")
            print(f"âœ… Inference {elapsed_time:.2f} saniyede tamamlandÄ±!")
            print(f"{'='*60}\n")
            return True, None, elapsed_time
        else:
            error_msg = f"Hata kodu: {result.returncode}\n\n"
            error_msg += f"STDOUT (son 2000 karakter):\n{result.stdout[-2000:]}\n\n"
            error_msg += f"STDERR (son 2000 karakter):\n{result.stderr[-2000:]}"
            print(f"âŒ {error_msg}")
            return False, error_msg, elapsed_time
        
    except subprocess.TimeoutExpired:
        error_msg = "â±ï¸ Ä°ÅŸlem zaman aÅŸÄ±mÄ±na uÄŸradÄ± (30 dakika)"
        print(f"âŒ {error_msg}")
        return False, error_msg, 0
    except Exception as e:
        import traceback
        error_msg = f"Hata: {str(e)}\n\n{traceback.format_exc()}"
        print(f"âŒ {error_msg}")
        return False, error_msg, 0

def restore_audio(audio_file, mode, n_steps, cutoff_freq_auto, cutoff_freq_manual, 
                  inpaint_length, use_fast_inference, precision, compile_mode):
    """Ana ses restorasyon fonksiyonu - hÄ±zlÄ± inference ile"""
    try:
        print("ğŸš€ BaÅŸlatÄ±lÄ±yor...")
        
        if audio_file is None:
            return None, "âŒ LÃ¼tfen bir ses dosyasÄ± yÃ¼kleyin!"
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        input_filename = os.path.basename(audio_file)
        base_name = input_filename.rsplit('.', 1)[0] if '.' in input_filename else input_filename
        output_filename = f"{timestamp}_{mode}_{base_name}.wav"
        output_path = os.path.join(OUTPUT_DIR, output_filename)
        
        print("ğŸ“Š Ses dosyasÄ± analiz ediliyor...")
        
        y, sr = librosa.load(audio_file, sr=None)
        duration = len(y) / sr
        
        info_text = f"## ğŸ“Š Ses Bilgileri\n\n"
        info_text += f"- **Sampling Rate:** {sr} Hz\n"
        info_text += f"- **Duration:** {duration:.2f} saniye\n"
        info_text += f"- **Samples:** {len(y):,}\n\n"
        
        # HÄ±zlÄ± inference ayarlarÄ±
        if use_fast_inference:
            info_text += f"## ğŸš€ Fast Inference AyarlarÄ±\n\n"
            info_text += f"- **Precision:** {precision}\n"
            info_text += f"- **Compile Mode:** {compile_mode}\n"
            info_text += f"- **torch.compile():** Etkin\n"
            info_text += f"- **cuDNN Benchmark:** Etkin\n"
            info_text += f"- **TF32:** Etkin\n\n"
        
        if IN_KAGGLE and duration > 60:
            info_text += f"âš ï¸ **UyarÄ±:** Ses dosyasÄ± {duration:.1f} saniye. Uzun iÅŸlemler zaman aÅŸÄ±mÄ±na uÄŸrayabilir.\n\n"
        
        base_config = os.path.join(SCRIPT_DIR, 'configs', 'ensemble_2split_sampling.yaml')
        
        if mode == "bandwidth":
            print("ğŸ¯ Bandwidth extension hazÄ±rlanÄ±yor...")
            
            if cutoff_freq_auto:
                cutoff_freq = compute_rolloff_freq(audio_file)
                info_text += f"ğŸ¯ **Otomatik Cutoff:** {cutoff_freq} Hz\n"
            else:
                cutoff_freq = int(cutoff_freq_manual)
                info_text += f"ğŸ¯ **Manuel Cutoff:** {cutoff_freq} Hz\n"
            
            info_text += f"âš™ï¸ **Sampling Steps:** {n_steps}\n\n"
            
            config_path = os.path.join(SCRIPT_DIR, 'configs', 'inference_files_upsampling.yaml')
            if not os.path.exists(config_path):
                return None, f"âŒ Config dosyasÄ± bulunamadÄ±: {config_path}"
            
            with open(config_path, 'r') as f:
                config = yaml.safe_load(f)
            
            config['data']['predict_filelist'] = [{
                'filepath': audio_file,
                'output_subdir': '.'
            }]
            
            config['data']['transforms_aug'][0]['init_args']['upsample_mask_kwargs'] = {
                'min_cutoff_freq': cutoff_freq,
                'max_cutoff_freq': cutoff_freq
            }
            
            if 'segment_length' in config['data']:
                config['data']['segment_length'] = 65280
            
            temp_config = os.path.join(SCRIPT_DIR, 'configs', f'temp_gradio_{timestamp}.yaml')
            with open(temp_config, 'w') as f:
                yaml.dump(config, f)
        
        elif mode == "inpainting":
            print("ğŸ¨ Audio inpainting hazÄ±rlanÄ±yor...")
            
            info_text += f"ğŸ¯ **Inpainting Length:** {inpaint_length}s\n"
            info_text += f"âš™ï¸ **Sampling Steps:** {n_steps}\n\n"
            
            config_path = os.path.join(SCRIPT_DIR, 'configs', 'inference_files_inpainting.yaml')
            if not os.path.exists(config_path):
                return None, f"âŒ Config dosyasÄ± bulunamadÄ±: {config_path}"
            
            with open(config_path, 'r') as f:
                config = yaml.safe_load(f)
            
            config['data']['predict_filelist'] = [{
                'filepath': audio_file,
                'output_subdir': '.'
            }]
            
            config['data']['transforms_aug'][0]['init_args']['inpainting_mask_kwargs'] = {
                'min_inpainting_frac': inpaint_length,
                'max_inpainting_frac': inpaint_length,
                'is_random': False
            }
            
            temp_config = os.path.join(SCRIPT_DIR, 'configs', f'temp_gradio_{timestamp}.yaml')
            with open(temp_config, 'w') as f:
                yaml.dump(config, f)
        
        print("ğŸ”„ HÄ±zlÄ± inference Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
        
        use_compile = use_fast_inference and torch.cuda.is_available()
        success, error, elapsed_time = run_inference_fast(
            [base_config, temp_config], 
            n_steps, 
            output_path,
            use_compile=use_compile,
            precision=precision if use_fast_inference else 'fp32',
            compile_mode=compile_mode
        )
        
        if os.path.exists(temp_config):
            os.remove(temp_config)
        
        if not success:
            return None, f"## âŒ Hata\n\n```\n{error}\n```"
        
        print("âœ¨ TamamlanÄ±yor...")
        
        if os.path.exists(output_path):
            info_text += f"---\n\n## âœ… Ä°ÅŸlem TamamlandÄ±!\n\n"
            info_text += f"â±ï¸ **Ä°ÅŸlem SÃ¼resi:** {elapsed_time:.2f} saniye\n"
            info_text += f"ğŸ“ **Ã‡Ä±ktÄ± DosyasÄ±:** `{output_filename}`\n"
            
            if IN_KAGGLE:
                info_text += f"ğŸ“ **Kaggle Yolu:** `/kaggle/working/gradio_outputs/{output_filename}`\n\n"
            else:
                info_text += f"\n"
            
            y_out, sr_out = librosa.load(output_path, sr=None)
            info_text += f"## ğŸ“Š Restore EdilmiÅŸ Ses\n\n"
            info_text += f"- **Sampling Rate:** {sr_out} Hz\n"
            info_text += f"- **Duration:** {len(y_out)/sr_out:.2f} saniye\n"
            info_text += f"- **Samples:** {len(y_out):,}\n\n"
            
            cent_orig = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr)[0])
            cent_rest = np.mean(librosa.feature.spectral_centroid(y=y_out, sr=sr_out)[0])
            rolloff_orig = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr, roll_percent=0.99)[0])
            rolloff_rest = np.mean(librosa.feature.spectral_rolloff(y=y_out, sr=sr_out, roll_percent=0.99)[0])
            
            info_text += f"## ğŸ“ˆ Spektral Analiz\n\n"
            info_text += f"| Ã–zellik | Orijinal | Restored | DeÄŸiÅŸim |\n"
            info_text += f"|---------|----------|----------|--------|\n"
            info_text += f"| Spectral Centroid | {cent_orig:.0f} Hz | {cent_rest:.0f} Hz | {((cent_rest-cent_orig)/cent_orig*100):+.1f}% |\n"
            info_text += f"| Spectral Rolloff | {rolloff_orig:.0f} Hz | {rolloff_rest:.0f} Hz | {((rolloff_rest-rolloff_orig)/rolloff_orig*100):+.1f}% |\n"
            
            if use_fast_inference:
                info_text += f"\n## âš¡ Performans\n\n"
                info_text += f"- **Fast Inference:** Etkin\n"
                info_text += f"- **Ä°ÅŸlem HÄ±zÄ±:** {duration/elapsed_time:.2f}x gerÃ§ek zamanlÄ±\n"
                info_text += f"- **Tahmini HÄ±zlanma:** 3-4x vs baseline\n"
            
            print("âœ… TamamlandÄ±!")
            return output_path, info_text
        else:
            return None, info_text + "\n---\n\n## âŒ Hata\n\nÃ‡Ä±ktÄ± dosyasÄ± oluÅŸturulamadÄ±."
    
    except Exception as e:
        import traceback
        error_msg = f"## âŒ Hata OluÅŸtu\n\n```\n{str(e)}\n\n{traceback.format_exc()}\n```"
        return None, error_msg

# ============================================================================
# GRADIO ARAYÃœZÃœ
# ============================================================================

# Sistem bilgisi
gpu_info = ""
if torch.cuda.is_available():
    gpu_name = torch.cuda.get_device_name(0)
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
    gpu_info = f"ğŸ® GPU: {gpu_name} ({gpu_memory:.1f} GB)"
else:
    gpu_info = "ğŸ–¥ï¸ CPU Modu"

pytorch_version = torch.__version__
fast_inference_available = tuple(map(int, pytorch_version.split('.')[:2])) >= (2, 0)

custom_css = """
.gradio-container {
    max-width: 1200px !important;
}
"""

with gr.Blocks(title="A2SB Fast Audio Restoration - Kaggle", theme=gr.themes.Soft(), css=custom_css, analytics_enabled=False) as demo:
    gr.Markdown(f"""
    # ğŸš€ A2SB: Fast Audio Restoration (Kaggle Edition)
    ### YÃ¼ksek Kaliteli Ses Restorasyonu - PyTorch OptimizasyonlarÄ± ile
    
    {gpu_info} | PyTorch {pytorch_version} | Fast Inference: {'âœ… Mevcut' if fast_inference_available else 'âš ï¸ PyTorch 2.0+ gerekli'}
    
    **Yeni Ã–zellikler:**
    - ğŸš€ 3-4x daha hÄ±zlÄ± inference (torch.compile())
    - âš¡ Mixed precision (FP16/BF16) desteÄŸi
    - ğŸ’¾ %50 daha az GPU bellek kullanÄ±mÄ±
    - ğŸ¯ Optimize edilmiÅŸ CUDA ayarlarÄ±
    """)
    
    if IN_KAGGLE:
        gr.Markdown("""
        ### ğŸ“Œ Kaggle KullanÄ±m NotlarÄ±:
        - â±ï¸ **HÄ±zlÄ± Mod:** 10 saniyelik ses ~1-1.5 dakika (P100 GPU)
        - ğŸ’¾ **Ã‡Ä±ktÄ±lar:** `/kaggle/working/gradio_outputs/` dizinine kaydedilir
        - ğŸ“¥ **Ä°ndirme:** ArayÃ¼zden veya Output sekmesinden indirebilirsiniz
        - ğŸš€ **Ä°lk Ã§alÄ±ÅŸtÄ±rma:** Model compile edilir (~1-2 dakika), sonraki Ã§alÄ±ÅŸtÄ±rmalar Ã§ok hÄ±zlÄ±
        """)
    
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### ğŸ“¤ GiriÅŸ")
            
            audio_input = gr.Audio(
                label="Ses DosyasÄ± YÃ¼kle",
                type="filepath",
                sources=["upload", "microphone"]
            )
            
            mode = gr.Radio(
                choices=["bandwidth", "inpainting"],
                value="bandwidth",
                label="Restorasyon Modu"
            )
            
            with gr.Accordion("âš™ï¸ Temel Ayarlar", open=True):
                n_steps = gr.Slider(
                    20, 100, 30, step=5, 
                    label="Sampling Steps (dÃ¼ÅŸÃ¼k = hÄ±zlÄ±)",
                    info="Ã–nerilen: 20-30 hÄ±zlÄ±, 50+ kaliteli"
                )
                cutoff_freq_auto = gr.Checkbox(True, label="Otomatik Cutoff")
                cutoff_freq_manual = gr.Slider(
                    1000, 10000, 2000, step=100, 
                    label="Manuel Cutoff (Hz)", 
                    visible=False
                )
                inpaint_length = gr.Slider(
                    0.1, 1.0, 0.3, step=0.1, 
                    label="Inpainting UzunluÄŸu (s)"
                )
            
            with gr.Accordion("ğŸš€ Fast Inference AyarlarÄ±", open=True):
                use_fast_inference = gr.Checkbox(
                    True, 
                    label="Fast Inference EtkinleÅŸtir (3-4x hÄ±zlanma)",
                    info="PyTorch 2.0+ ve CUDA gerektirir"
                )
                precision = gr.Radio(
                    choices=["fp16", "bf16", "fp32"],
                    value="fp16",
                    label="Precision",
                    info="fp16=en hÄ±zlÄ±, fp32=en kaliteli"
                )
                compile_mode = gr.Radio(
                    choices=["reduce-overhead", "max-autotune", "default"],
                    value="reduce-overhead",
                    label="Compile Modu",
                    info="max-autotune=en hÄ±zlÄ± (ilk Ã§alÄ±ÅŸtÄ±rma uzun)"
                )
            
            cutoff_freq_auto.change(
                fn=lambda x: gr.update(visible=not x),
                inputs=[cutoff_freq_auto],
                outputs=[cutoff_freq_manual]
            )
            
            restore_btn = gr.Button("ğŸš€ Restore Et", variant="primary", size="lg")
            
            if IN_KAGGLE:
                gr.Markdown("""
                ### ğŸ’¡ Ä°puÃ§larÄ±:
                - Maksimum hÄ±z: 20-30 step, FP16
                - Dengeli: 40-50 step, FP16
                - Maksimum kalite: 80-100 step, FP32
                - P100 GPU seÃ§in (Settings > Accelerator)
                """)
        
        with gr.Column(scale=1):
            gr.Markdown("### ğŸ“¥ Ã‡Ä±ktÄ±")
            audio_output = gr.Audio(label="Restore EdilmiÅŸ Ses", type="filepath")
            info_output = gr.Markdown("Ses dosyasÄ± yÃ¼kleyin ve 'Restore Et' butonuna tÄ±klayÄ±n.")
    
    restore_btn.click(
        fn=restore_audio,
        inputs=[audio_input, mode, n_steps, cutoff_freq_auto, cutoff_freq_manual, 
                inpaint_length, use_fast_inference, precision, compile_mode],
        outputs=[audio_output, info_output]
    )
    
    gr.Markdown("""
    ---
    ### ğŸ“– Kaynak
    - ğŸ“„ [Paper](https://arxiv.org/abs/2501.11311)
    - ğŸ’» [GitHub](https://github.com/test4373/diffusion-audio-restoration-colab-Kaggle-.git)
    - ğŸ¤— [Models](https://huggingface.co/nvidia/audio_to_audio_schrodinger_bridge)
    """)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='A2SB Fast Gradio App')
    parser.add_argument('--share', action='store_true', help='Create public URL')
    parser.add_argument('--port', type=int, default=7860, help='Port number')
    parser.add_argument('--ngrok', action='store_true', help='Use ngrok tunnel')
    
    try:
        args = parser.parse_args()
    except SystemExit:
        args = argparse.Namespace(share=False, port=7860, ngrok=False)
    
    print("\n" + "="*60)
    print("ğŸš€ A2SB Fast Audio Restoration (Kaggle)")
    print("="*60)
    
    print(f"\nPyTorch: {pytorch_version}")
    if torch.cuda.is_available():
        print(f"CUDA: {torch.version.cuda}")
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB")
    else:
        print("CPU Modu")
    
    if fast_inference_available:
        print("âœ… Fast inference mevcut (PyTorch 2.0+)")
    else:
        print("âš ï¸ Fast inference PyTorch 2.0+ gerektirir")
    
    # Ngrok setup
    ngrok_tunnel = None
    if IN_KAGGLE or args.ngrok:
        try:
            from pyngrok import ngrok as pyngrok_module
            
            print("ğŸŒ Kaggle ortamÄ± - Ngrok tunnel oluÅŸturuluyor...")
            
            ngrok_token = os.environ.get('NGROK_TOKEN', '')
            if ngrok_token:
                pyngrok_module.set_auth_token(ngrok_token)
                print("âœ… Ngrok token ayarlandÄ±")
            
            ngrok_tunnel = pyngrok_module.connect(args.port)
            public_url = ngrok_tunnel.public_url
            
            print(f"\nâœ… Ngrok tunnel oluÅŸturuldu!")
            print(f"ğŸ”— Public URL: {public_url}\n")
            
            share = True
        except Exception as e:
            print(f"âš ï¸ Ngrok hatasÄ±: {e}")
            share = True
    else:
        share = args.share
    
    print("\nğŸš€ Gradio baÅŸlatÄ±lÄ±yor...")
    print("="*60 + "\n")
    
    try:
        demo.launch(
            share=True,
            server_port=args.port,
            debug=False,
            show_error=True,
            show_api=False,
            server_name="0.0.0.0" if IN_KAGGLE else "127.0.0.1",
            quiet=False,
            prevent_thread_lock=False,
            inbrowser=False,
            allowed_paths=[OUTPUT_DIR] if IN_KAGGLE else None
        )
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ Uygulama kapatÄ±lÄ±yor...")
        if ngrok_tunnel:
            pyngrok_module.disconnect(ngrok_tunnel.public_url)
    except Exception as e:
        print(f"\nâŒ Hata: {e}")
        if ngrok_tunnel:
            pyngrok_module.disconnect(ngrok_tunnel.public_url)
